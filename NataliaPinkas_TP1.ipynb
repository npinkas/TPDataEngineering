{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c866524",
   "metadata": {},
   "source": [
    "## Extracción y almacenamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d299f2",
   "metadata": {},
   "source": [
    "### Justificación de elección de API:\n",
    "\n",
    "En el presente trabajo decidi elegir la API JCDecaux Developers ya que la misma ofrece datos estáticos como la información fija de cada contrato particular, los cuales serán utiles para realizar una extracción full. Asimismo, ofrece datos dinámicos como la información en tiempo real de las estaciones de bicicletas de un contrato particular, por lo que resulta de gran utilidad para la extracción incremental. En este sentido, considero importante aclarar que para este trabajo utilice dos enpoints:\n",
    "- Extracción full: Obtuve los metadatos de las estaciones de bicicletas.\n",
    "- Extracción incremental: Utilice información en tiempo real sobre el estado de cada estación, la cual incluye las bicicletas disponibles, si se debe pagar, si se encuentra abierto, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920867e",
   "metadata": {},
   "source": [
    "### Desarrollo del trabajo práctico con sus respectivos comentarios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676d3de",
   "metadata": {},
   "source": [
    "##### Instalación de librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88849a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install deltalake\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e1960",
   "metadata": {},
   "source": [
    "##### Importación de librerias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393191e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "from deltalake import write_deltalake, DeltaTable\n",
    "from deltalake.exceptions import TableNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62411f6d",
   "metadata": {},
   "source": [
    "##### Definición de funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebdeda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Manejo de json\n",
    "\n",
    "def get_last_update (file_path):\n",
    "\n",
    "    try:\n",
    "        #with es util porque permite simplificar el manejo de archivos, conexiones, db al asegurarse que se usan y liberan de forma correcta, incluso si hay errores.\n",
    "        \n",
    "        with open(file_path, \"r\") as file: \n",
    "            ultimoUpdate = json.load(file) # leo el archivo json\n",
    "            return ultimoUpdate[\"last_updated\"]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"El archivo JSON en la ruta {file_path} no existe.\") # propago el error y lo resuelvo en otro lado!\n",
    "    \n",
    "    except json.JSONDecodeError:\n",
    "        raise json.JSONDecodeError(f\"El archivo JSON en la ruta {file_path} no es válido.\")\n",
    "\n",
    "    except KeyError as k:\n",
    "        raise KeyError(str(k))\n",
    "\n",
    "\n",
    "def update_last_update (file_path, last_updated):\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"w\") as file:\n",
    "            json.dump({\"last_updated\": last_updated.isoformat()}, file, indent=4)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"El archivo JSON en la ruta {file_path} no existe.\") # propago el error y lo resuelvo en otro lado!\n",
    "    \n",
    "# Extraccion de datos\n",
    "\n",
    "def get_data_stations(url, params=None):\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url,params=params, timeout = 10)\n",
    "        response.raise_for_status() # excepcion que captura el except\n",
    "        data = response.json()\n",
    "        return create_data_frame(data)\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error al obtener los datos. Código de error: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_data_frame(json_data):\n",
    "\n",
    "    try:\n",
    "        df = pd.json_normalize(json_data)\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Se produjo un error en la construcción del DataFrame: {e}\")   \n",
    "        return pd.DataFrame() \n",
    "    \n",
    "def incremental_extraction(url, file_path, params=None):\n",
    "\n",
    "    try:\n",
    "\n",
    "        ultimo_updateJson = get_last_update(file_path=file_path)\n",
    "        ultimo_update = pd.to_datetime(ultimo_updateJson, utc=True)\n",
    "\n",
    "        df = get_data_stations(url, params=params)\n",
    "        if df is None:\n",
    "            print(\"No se pudo construir el DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        fechas_convertidas = []\n",
    "        for f in df[\"last_update\"]:\n",
    "            if f is not None and not math.isnan(f):\n",
    "                ft = datetime.fromtimestamp(f / 1000, tz=timezone.utc)\n",
    "                fechas_convertidas.append(ft)\n",
    "            else:\n",
    "                fechas_convertidas.append(pd.NaT) #fecha nula en caso de ser null\n",
    "            \n",
    "        df[\"last_update\"] = fechas_convertidas\n",
    "\n",
    "        df_incremental = df[df[\"last_update\"] > ultimo_update]\n",
    "\n",
    "        if df_incremental.empty:\n",
    "            print(\"No hay nuevas actualizaciones desde la última consulta\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        max_timestamp = df[\"last_update\"].max()\n",
    "        update_last_update(file_path=file_path, last_updated=max_timestamp)\n",
    "        return df_incremental\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Se produjo un error en la extracción incremental {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Almacenamiento de datos\n",
    "\n",
    "def save_data_as_delta(df, path, mode=\"overwrite\", partition_cols=None):\n",
    "\n",
    "    write_deltalake(path, df, mode = mode, partition_by = partition_cols)\n",
    "\n",
    "def merge_new_data_as_delta(data, data_path, predicate):\n",
    "\n",
    "    try:\n",
    "        dt = DeltaTable(data_path)\n",
    "        data_pa = pa.Table.from_pandas(data)\n",
    "        dt.merge(\n",
    "            source=data_pa,\n",
    "            source_alias=\"source\",\n",
    "            target_alias=\"target\",\n",
    "            predicate=predicate\n",
    "        ) \\\n",
    "        .when_matched_update_all() \\\n",
    "        .when_not_matched_insert_all() \\\n",
    "        .execute()\n",
    "    except TableNotFoundError:\n",
    "        save_data_as_delta(data, data_path)\n",
    "    \n",
    "def save_new_data_as_delta(new_data, data_path, predicate, partition_cols=None):\n",
    "   \n",
    "    try:\n",
    "      dt = DeltaTable(data_path)\n",
    "      new_data_pa = pa.Table.from_pandas(new_data)\n",
    "\n",
    "      # Se insertan en target, datos de source que no existen en target\n",
    "      dt.merge(\n",
    "          source=new_data_pa,\n",
    "          source_alias=\"source\",\n",
    "          target_alias=\"target\",\n",
    "          predicate=predicate\n",
    "      ) \\\n",
    "      .when_not_matched_insert_all() \\\n",
    "      .execute()\n",
    "\n",
    "    # Si no existe la tabla Delta Lake, se guarda como nueva\n",
    "    except TableNotFoundError:\n",
    "      save_data_as_delta(new_data, data_path, partition_cols=partition_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af00c8c",
   "metadata": {},
   "source": [
    "#### Extracción Full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b51fd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Obtengo api-keys\n",
    "\n",
    "parser = ConfigParser()\n",
    "parser.read(\"pipeline.conf\")\n",
    "api_key = parser[\"api-credentials\"][\"api-key\"]         \n",
    "\n",
    "urlBase = \"https://api.jcdecaux.com/vls/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b39ba0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "paramsEstatico = {\n",
    "        \"apiKey\" : api_key\n",
    "    }\n",
    "\n",
    "endpointEstatico = \"contracts\"\n",
    "\n",
    "urlEstatica = f\"{urlBase}/{endpointEstatico}\"\n",
    "    \n",
    "df_estatico = utils.obtenerDatosEstaciones(urlEstatica, paramsEstatico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8faf954",
   "metadata": {},
   "source": [
    "#### Extracción Incremental:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41206e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "endpointDinamico = \"stations\"\n",
    "\n",
    "paramsDinamico = {\n",
    "    \"contract\": \"lyon\",\n",
    "    \"apiKey\" : api_key\n",
    "}\n",
    "\n",
    "urlDinamica= f\"{urlBase}/{endpointDinamico}\"\n",
    "\n",
    "df_dinamico = utils.extraccionIncremental(urlDinamica, \"metadata/metadata.json\", paramsDinamico)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231cc57",
   "metadata": {},
   "source": [
    "#### Almacenamiento de Datos - Extracción Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca56e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "contracts_dir = f\"{bronze_dir}/{endpointEstatico}\"\n",
    "\n",
    "utils.merge_new_data_as_delta(df_estatico, contracts_dir, \"target.name = source.name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20caf6fc",
   "metadata": {},
   "source": [
    "#### Almacenamiento de Datos - Extracción Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3243ae7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stations_dir = f\"{bronze_dir}/{endpointDinamico}\"\n",
    "\n",
    "if not df_dinamico.empty:\n",
    "\n",
    "    df_dinamico[\"last_update\"] = utils.pd.to_datetime(df_dinamico.last_update)\n",
    "    \n",
    "    df_dinamico[\"fecha\"] = df_dinamico.last_update.dt.date\n",
    "    \n",
    "    df_dinamico[\"hora\"] = df_dinamico.last_update.dt.hour\n",
    "\n",
    "    utils.save_new_data_as_delta(df_dinamico, stations_dir, predicate= \"target.number = source.number AND target.last_update = source.last_update\", partition_cols=[\"fecha\", \"hora\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
