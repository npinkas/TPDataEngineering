{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c866524",
      "metadata": {
        "id": "9c866524"
      },
      "source": [
        "## Extracción y almacenamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25d299f2",
      "metadata": {
        "id": "25d299f2"
      },
      "source": [
        "### Justificación de elección de API:\n",
        "\n",
        "Para este trabajo decidí utilizar la API de JCDecaux Developers porque ofrece datos estáticos y dinámicos. Por un lado, proporciona información estática sobre cada contrato, lo cual es útil para realizar una extracción full que obtenga los metadatos de las estaciones de bicicletas. Por otro lado, ofrece datos en tiempo real sobre el estado de cada estación, incluyendo bicicletas disponibles, si se debe pagar, si está abierta, entre otros, los cuales se pueden obtener mediante extracción incremental. A partir de esto, utilcé dos endpoints en particular:\n",
        "- **Extracción full:** Elegí el endpoint que ofrece los metadatos estáticos de las estaciones de cada contrato, ya que tiene gran cantidad de información que se puede limpiar y procesar luego.\n",
        "- **Extracción incremental:** Seleccioné un endpoint que me permitió recolectar la información en tiempo real de cada estación, siendo útil para analizar tendencias futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6920867e",
      "metadata": {
        "id": "6920867e"
      },
      "source": [
        "### Desarrollo del trabajo práctico:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6676d3de",
      "metadata": {
        "id": "6676d3de"
      },
      "source": [
        "##### Instalación de librerias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa88849a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa88849a",
        "outputId": "2ee86b57-c6e9-4459-9289-748673dac509"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "!pip install deltalake\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948e1960",
      "metadata": {
        "id": "948e1960"
      },
      "source": [
        "##### Importación de librerias y módulos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e393191e",
      "metadata": {
        "id": "e393191e"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import math\n",
        "from datetime import datetime, timezone\n",
        "from deltalake import write_deltalake, DeltaTable\n",
        "from deltalake.exceptions import TableNotFoundError\n",
        "from configparser import ConfigParser\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62411f6d",
      "metadata": {
        "id": "62411f6d"
      },
      "source": [
        "##### Definición de funciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bebdeda",
      "metadata": {
        "id": "5bebdeda"
      },
      "outputs": [],
      "source": [
        "# Manejo de json\n",
        "\n",
        "def get_last_update (file_path):\n",
        "\n",
        "    try:\n",
        "        #with es util porque permite simplificar el manejo de archivos, conexiones, db al asegurarse que se usan y liberan de forma correcta, incluso si hay errores.\n",
        "\n",
        "        with open(file_path, \"r\") as file:\n",
        "            ultimo_update = json.load(file) # leo el archivo json\n",
        "            return ultimo_update[\"last_updated\"]\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"El archivo JSON en la ruta {file_path} no existe.\") # propago el error y lo resuelvo en otro lado!\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        raise json.JSONDecodeError(f\"El archivo JSON en la ruta {file_path} no es válido.\")\n",
        "\n",
        "    except KeyError as k:\n",
        "        raise KeyError(str(k))\n",
        "\n",
        "\n",
        "def update_last_update (file_path, last_updated):\n",
        "\n",
        "    try:\n",
        "        with open(file_path, \"w\") as file:\n",
        "            json.dump({\"last_updated\": last_updated.isoformat()}, file, indent=4)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"El archivo JSON en la ruta {file_path} no existe.\") # propago el error y lo resuelvo en otro lado!\n",
        "\n",
        "# Extraccion de datos\n",
        "\n",
        "def get_data_stations(url, params=None):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url,params=params, timeout = 10)\n",
        "        response.raise_for_status() # excepcion que captura el except\n",
        "        data = response.json()\n",
        "        return create_data_frame(data)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error al obtener los datos. Código de error: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def create_data_frame(json_data):\n",
        "\n",
        "    try:\n",
        "        df = pd.json_normalize(json_data)\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Se produjo un error en la construcción del DataFrame: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def incremental_extraction(url, file_path, params=None):\n",
        "\n",
        "    try:\n",
        "\n",
        "        last_update_json = get_last_update(file_path=file_path)\n",
        "        last_update = pd.to_datetime(last_update_json, utc=True)\n",
        "\n",
        "        df = get_data_stations(url, params=params)\n",
        "        if df is None:\n",
        "            print(\"No se pudo construir el DataFrame.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        fechas_convertidas = []\n",
        "        for f in df[\"last_update\"]:\n",
        "            if f is not None and not math.isnan(f):\n",
        "                ft = datetime.fromtimestamp(f / 1000, tz=timezone.utc)\n",
        "                fechas_convertidas.append(ft)\n",
        "            else:\n",
        "                fechas_convertidas.append(pd.NaT) #fecha nula en caso de ser null\n",
        "\n",
        "        df[\"last_update\"] = fechas_convertidas\n",
        "\n",
        "        df_incremental = df[df[\"last_update\"] > last_update]\n",
        "\n",
        "        if df_incremental.empty:\n",
        "            print(\"No hay nuevas actualizaciones desde la última consulta\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        max_timestamp = df[\"last_update\"].max()\n",
        "        update_last_update(file_path=file_path, last_updated=max_timestamp)\n",
        "        return df_incremental\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Se produjo un error en la extracción incremental {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "# Almacenamiento de datos\n",
        "\n",
        "def save_data_as_delta(df, path, mode=\"overwrite\", partition_cols=None):\n",
        "\n",
        "    write_deltalake(path, df, mode = mode, partition_by = partition_cols)\n",
        "\n",
        "def merge_new_data_as_delta(new_data, data_path, predicate, partition_cols=None):\n",
        "\n",
        "    try:\n",
        "        dt = DeltaTable(data_path)\n",
        "        data_pa = pa.Table.from_pandas(new_data)\n",
        "        dt.merge(\n",
        "            source=data_pa,\n",
        "            source_alias=\"source\",\n",
        "            target_alias=\"target\",\n",
        "            predicate=predicate\n",
        "        ) \\\n",
        "        .when_matched_update_all() \\\n",
        "        .when_not_matched_insert_all() \\\n",
        "        .execute()\n",
        "    except TableNotFoundError:\n",
        "      save_data_as_delta(new_data, data_path, partition_cols=partition_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af00c8c",
      "metadata": {
        "id": "3af00c8c"
      },
      "source": [
        "#### Extracción Full:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb22713",
      "metadata": {},
      "source": [
        "Los metadatos de las estaciones se obtienen mediante una extracción full, ya que son datos estáticos que cambian muy poco o casi nunca. Por eso, no es necesario actualizarlos constantemente y una extracción full realizada con cierta frecuencia basta para mantenerlos actualizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b51fd0",
      "metadata": {
        "id": "d6b51fd0"
      },
      "outputs": [],
      "source": [
        "# Obtengo api-keys\n",
        "\n",
        "parser = ConfigParser()\n",
        "parser.read(\"pipeline.conf\")\n",
        "api_key = parser[\"api-credentials\"][\"api-key\"]\n",
        "\n",
        "urlBase = \"https://api.jcdecaux.com/vls/v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b39ba0",
      "metadata": {
        "id": "b2b39ba0"
      },
      "outputs": [],
      "source": [
        "paramsEstatico = {\n",
        "        \"apiKey\" : api_key\n",
        "    }\n",
        "\n",
        "endpointEstatico = \"contracts\"\n",
        "\n",
        "urlEstatica = f\"{urlBase}/{endpointEstatico}\"\n",
        "\n",
        "df_estatico = get_data_stations(urlEstatica, paramsEstatico)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8faf954",
      "metadata": {
        "id": "e8faf954"
      },
      "source": [
        "#### Extracción Incremental:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_klR7zCk1Cv",
      "metadata": {
        "id": "m_klR7zCk1Cv"
      },
      "source": [
        "En primer lugar, utilicé un método stateful, es decir, almacené el último update en un archivo JSON. Esto se debe a que los datos se actualizan en tiempo real, por lo que de esta forma siempre se obtiene solo la información nueva o modificada desde la última consulta. Para la extracción incremental, se consulta ese último update guardado y se solicitan únicamente los datos que superen ese estado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41206e0",
      "metadata": {
        "id": "e41206e0"
      },
      "outputs": [],
      "source": [
        "endpointDinamico = \"stations\"\n",
        "\n",
        "paramsDinamico = {\n",
        "    \"contract\": \"lyon\",\n",
        "    \"apiKey\" : api_key\n",
        "}\n",
        "\n",
        "urlDinamica= f\"{urlBase}/{endpointDinamico}\"\n",
        "\n",
        "path_metadata = \"metadata/metadata.json\"\n",
        "\n",
        "df_dinamico = incremental_extraction(urlDinamica, path_metadata, paramsDinamico)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f231cc57",
      "metadata": {
        "id": "f231cc57"
      },
      "source": [
        "#### Almacenamiento de Datos - Extracción Full"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9FRI1MkIkf8E",
      "metadata": {
        "id": "9FRI1MkIkf8E"
      },
      "source": [
        "En la extracción full, se sobreescriben todos los datos directamente para evitar la existencia de registros duplicados. Esto garantiza que siempre podamos ver el estado más actualizado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2yUptcigJUxy",
      "metadata": {
        "id": "2yUptcigJUxy"
      },
      "outputs": [],
      "source": [
        "bronze_dir = \"datalake/bronze/jcdecauxDeveloper\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ca56e5",
      "metadata": {
        "id": "87ca56e5"
      },
      "outputs": [],
      "source": [
        "contracts_dir = f\"{bronze_dir}/{endpointEstatico}\"\n",
        "\n",
        "merge_new_data_as_delta(df_estatico, contracts_dir, \"target.name = source.name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZA5-8E5ght16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        },
        "id": "ZA5-8E5ght16",
        "outputId": "192e7484-7638-41c6-c600-0534d4dbe2d5"
      },
      "outputs": [],
      "source": [
        "dt = DeltaTable(\"datalake/bronze/jcdecauxDeveloper/contracts\")\n",
        "df = dt.to_pandas()\n",
        "df.sort_values(by= \"name\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20caf6fc",
      "metadata": {
        "id": "20caf6fc"
      },
      "source": [
        "#### Almacenamiento de Datos - Extracción Incremental"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fW0X7oueakli",
      "metadata": {
        "id": "fW0X7oueakli"
      },
      "source": [
        "Decidí registrar todos los datos que obtengo de la API en tiempo real, guardando un historial completo del estado de cada estación en cada fecha específica. Es decir, las actualizaciones se realizan considerando cada fecha, evitando duplicados por fecha. Si bien esto puede generar algunos datos repetidos, creo que es una estrategia adecuada porque me permite conocer el estado de cada estación en cada momento. Esto facilita analizar cambios a lo largo del tiempo y detectar tendencias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3243ae7a",
      "metadata": {
        "id": "3243ae7a"
      },
      "outputs": [],
      "source": [
        "stations_dir = f\"{bronze_dir}/{endpointDinamico}\"\n",
        "\n",
        "if not df_dinamico.empty:\n",
        "\n",
        "    df_dinamico[\"last_update\"] = pd.to_datetime(df_dinamico.last_update)\n",
        "\n",
        "    df_dinamico[\"fecha\"] = df_dinamico.last_update.dt.date\n",
        "\n",
        "    merge_new_data_as_delta(df_dinamico, stations_dir, predicate= \"target.number = source.number AND target.contract_name = source.contract_name AND target.fecha = source.fecha\", partition_cols=[\"fecha\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hk-Kvr1vXAb9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "Hk-Kvr1vXAb9",
        "outputId": "837c99fb-e15c-4a81-c5aa-a4c8a41753e9"
      },
      "outputs": [],
      "source": [
        "dt = DeltaTable(stations_dir)\n",
        "df = dt.to_pandas()\n",
        "df.sort_values(by= \"number\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ffe9eae",
      "metadata": {},
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de1880fe",
      "metadata": {},
      "source": [
        "## Procesamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85eda89a",
      "metadata": {},
      "source": [
        "#### Importación de librerías y módulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2f2aa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf94afcf",
      "metadata": {},
      "source": [
        "#### Definición de funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72430e69",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_most_recent_partition(data_path, file_path):\n",
        "    \n",
        "    try:\n",
        "      last_update_json = get_last_update(file_path=file_path)\n",
        "      requested_date  = pd.to_datetime(last_update_json, utc=True)\n",
        "      dt = DeltaTable(data_path)\n",
        "      df_recent = dt.to_pandas(\n",
        "        partitions=[\n",
        "        (\"fecha\", \"=\", requested_date.date())]\n",
        "        )\n",
        "      return df_recent\n",
        "    except Exception as E:\n",
        "      raise Exception(f\"No se pudo procesar la tabla Delta Lake, por {E}\")\n",
        "      return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75826886",
      "metadata": {},
      "source": [
        "#### Obtención de datos de la capa Bronze - Extracción full"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9950431b",
      "metadata": {},
      "source": [
        "En primer lugar, debo obtener los datos crudos almacenados en la capa Bronze. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d369c28a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_estatico_bronze = DeltaTable(contracts_dir).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef971a29",
      "metadata": {},
      "source": [
        "#### Obtención de datos de la capa Bronze - Extracción incremental"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8768b5c7",
      "metadata": {},
      "source": [
        "En el caso de los datos dinámicos, los mismos se encuentran almacenados en distintas particiones en la capa bronze. Por ende, debo leer la última partición obtenida utilizando la función read_most_recent_partition()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef9d374",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze = read_most_recent_partition(stations_dir, path_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eec9f9a",
      "metadata": {},
      "source": [
        "#### Transformaciones - Extracción Full"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f565be9f",
      "metadata": {},
      "source": [
        "##### 1. Eliminación de duplicados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f3f49b",
      "metadata": {},
      "source": [
        "En primer lugar garantizo que no haya duplicados en los datos provenientes del endpoint estático. Para determinar si las filas están duplicadas me baso en el nombre del contrato, ya que el mismo es el que identifica unívocamente a cada fila en este caso. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75074f2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_estatico_bronze_cleaned = df_estatico_bronze.drop_duplicates(subset=\"name\", keep=\"first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "797751ee",
      "metadata": {},
      "source": [
        "##### 2. Eliminación o reemplazo de nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794b8f2d",
      "metadata": {},
      "source": [
        "En el caso de los datos obtenidos del endpoint estático, se imputarán con un valor por defecto todas las filas que contengan valores nulos excepto el nombre del contrato. En caso de que el mismo sea null se eliminará la fila, ya que el mismo es el que identifica a cada contrato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee43f70e",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned.dropna(subset=[\"name\"])\n",
        "\n",
        "imputation_mapping_e = {\n",
        "    \"commercial_name\": \"No especificado\",\n",
        "    \"cities\": \"No especificado\",\n",
        "    \"country_code\": \"No especificado\"\n",
        "}\n",
        "\n",
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned.fillna(imputation_mapping_e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1916320",
      "metadata": {},
      "source": [
        "##### 3. Eliminación de campos multivaluados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa842aa",
      "metadata": {},
      "source": [
        "Con el fin de evitar tener campos multivaluados, decidí crear un registro por cada una de las ciudades que conforman la lista de ciudades del endpoint estático."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91c2dba",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned[[\"name\", \"commercial_name\", \"cities\", \"country_code\"]].copy()\n",
        "\n",
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned.explode(\"cities\")\n",
        "\n",
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned.rename(columns={\"cities\": \"city\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf81b4fa",
      "metadata": {},
      "source": [
        "##### 4. Conversión de tipos de datos de columnas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91276988",
      "metadata": {},
      "source": [
        "A continuación, se realizan las conversiones de tipos de datos de columnas del endpoint estático, con el objetivo de evitar ocupar más espacio del necesario y establecer el tipo adecuado para cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540e9d48",
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_mapping_e = {\n",
        "    \"name\": \"string\",\n",
        "    \"commercial_name\": \"string\",\n",
        "    \"city\": \"string\",\n",
        "    \"country_code\": \"category\"\n",
        "\n",
        "}\n",
        "\n",
        "df_estatico_bronze_cleaned = df_estatico_bronze_cleaned.astype(conversion_mapping_e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc2d6ed8",
      "metadata": {},
      "source": [
        "#### Transformaciones - Extracción Incremental"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee78f35",
      "metadata": {},
      "source": [
        "##### 1. Eliminación de duplicados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b04d591",
      "metadata": {},
      "source": [
        "Respecto a los datos provenientes del endpoint dinamico, me baso en el número de la estación y el nombre del contrato (cada número de estación es único dentro de un contrato particular)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dacd860",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze_cleaned = df_dinamico_bronze.drop_duplicates(subset=[\"contract_name\", \"number\", \"last_update\"], keep=\"first\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d0b747",
      "metadata": {},
      "source": [
        "##### 2. Eliminación o reemplazo de nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb74100",
      "metadata": {},
      "source": [
        "Del mismo modo, los datos obtenidos del endpoint dinámico se imputarán con un valor por defecto, excepto por el número de la estación y el nombre del contrato ya que ambos son parte de la clave primaria que sirve para identificar cada estación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e48dd7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze_cleaned = df_dinamico_bronze_cleaned.dropna(subset=[\"number\", \"contract_name\"])\n",
        "\n",
        "imputation_mapping_d = {\n",
        "    \"name\": \"No especificado\",\n",
        "    \"address\": \"No especificado\",\n",
        "    \"position\": \"No especificado\",\n",
        "    \"banking\": \"No especificado\",\n",
        "    \"bonus\": \"No especificado\",\n",
        "    \"bike_stands\": 0,\n",
        "    \"available_bike_stands\": 0,\n",
        "    \"available_bikes\": 0,\n",
        "    \"status\": \"No especificado\"\n",
        "}\n",
        "\n",
        "df_dinamico_bronze_cleaned = df_dinamico_bronze_cleaned.fillna(imputation_mapping_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47413bc3",
      "metadata": {},
      "source": [
        "##### 3. Creación de nuevas columnas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50175835",
      "metadata": {},
      "source": [
        "Para el caso del endpoint dinámico renombré las columnas de latitud y longitud para que en vez de aparecer position.lat aparezca position_lat, por ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "794c1284",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze_cleaned = df_dinamico_bronze_cleaned.rename(columns=lambda col: col.replace(\".\",\"_\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13976cf1",
      "metadata": {},
      "source": [
        "##### 4. Mejora en la legibilidad de los campos banking y bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f4839e",
      "metadata": {},
      "source": [
        "Con el fin de mejorar la expresividad de los valores de las columnas banking y bonus, decidí reemplazar los valores True y False por una sentencia más clara para el usuario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b3f3a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze_cleaned[\"banking\"] = df_dinamico_bronze_cleaned[\"banking\"].replace({True: \"Payment required\", False: \"No payment required\"})\n",
        "df_dinamico_bronze_cleaned[\"bonus\"] = df_dinamico_bronze_cleaned[\"bonus\"].replace({True: \"Bonus station\", False: \"No bonus station\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05971e31",
      "metadata": {},
      "source": [
        "##### 5. Eliminación de campos vacios o blank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6a52d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "df_dinamico_bronze_cleaned = df_dinamico_bronze_cleaned.replace({\"\": \"No especificado\", \" \": \"No especificado\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c13e3c",
      "metadata": {},
      "source": [
        "##### 6. Conversión de tipos de datos de columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a26f7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_mapping_d = {\n",
        "    \"number\": \"int16\",\n",
        "    \"contract_name\": \"string\",\n",
        "    \"address\": \"string\",\n",
        "    \"position_lat\": \"float32\",\n",
        "    \"position_lng\": \"float32\",\n",
        "    \"bike_stands\": \"int8\",\n",
        "    \"available_bike_stands\": \"int8\",\n",
        "    \"available_bikes\": \"int8\",\n",
        "    \"status\": \"category\",\n",
        "    \"last_update\": \"datetime64[ms, UTC]\",\n",
        "    \"banking\": \"string\",\n",
        "    \"bonus\": \"string\"\n",
        "}\n",
        "\n",
        "df_dinamico_bronze_cleaned = df_dinamico_bronze_cleaned.astype(conversion_mapping_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b88032",
      "metadata": {},
      "source": [
        "#### Almacenamiento en capa Silver - Extracción Full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e2d276",
      "metadata": {},
      "outputs": [],
      "source": [
        "silver_dir = \"datalake/silver/jcdecauxDeveloper\"\n",
        "\n",
        "contracts_dir_silver = f\"{silver_dir}/contracts\"\n",
        "\n",
        "merge_new_data_as_delta(df_estatico_bronze_cleaned, contracts_dir_silver, predicate=\"target.name = source.name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c972c7ef",
      "metadata": {},
      "source": [
        "#### Almacenamiento en capa Silver - Extracción Incremental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb530afe",
      "metadata": {},
      "outputs": [],
      "source": [
        "stations_dir_silver = f\"{silver_dir}/stations\"\n",
        "\n",
        "merge_new_data_as_delta(df_dinamico_bronze_cleaned, stations_dir_silver, predicate=\"target.number = source.number AND target.fecha = source.fecha AND target.contract_name=source.contract_name\", partition_cols=[\"fecha\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tpIntegrador (3.11.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
